<style>
.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}
</style>

NextWord Suggester
========================================================
author: jrfoster
date: 20 February, 2017
autosize: true
transition: rotate

Background
========================================================
There are a number of instances of text and word suggestion in popular consumer and business technology today:
 - SMS and instant messaging on smart phones
 - Search engine query input

As part of the JHU Data Science Capstone, this application presents a word suggestor for phrases based on a corpus of over 4 million documents consistign of blog and news articles and tweets read from the Twitter API.

The application is driven by a persistent n-gram model consisting of unigrams up to quintagrams and employs Stupid Backoff smoothing.

Data Processing and Model Generation
========================================================
Persistent model was created using the following general steps:
 - A random training set of 75% of news and blog articles and 25% of tweets was derived using rbinom
 - Numbers, puntuation, symbols, separators, hyphens, and urls were removed and each document loaded into a Quanteda corpus
 - Map-reduce process used to generate n-grams using Quanteda for tokenizataion and Microsoft SQL Server for reducing
 - N-grams were pruned based on n-gram frequency
 - SQL Server Integratration Services and SQLite to create the persistent model

Prediction Algorithm Overview
========================================================
Generally speaking the algorithm used is as follows:
 - Based on the input length, candidate n-grams are retrieved
 - N-grams are scored by calculating relative frequency (see Stanford SLP)
 - Backoff multiplier is applied depending on the rank of the n-gram (see Brants)
 - Highest ranked and scored words are returned as predictions
 - If not enough suitable words are found the top unigrams are suggested


Application Overview
========================================================
The application has a relatively simple interface

![screenshot](./screenshot.png)

References
========================================================
Much of the work for this application is based on the following works:

<a href="http://www.aclweb.org/anthology/D07-1090.pdf" target="_blank">Large Language Models in Machine Translation (Brants, et al)</a><br/>
<a href="http://onepager.togaware.com/TextMiningO.pdf" target="_blank">Hands-On Data Science with R Text Mining (Williams)</a><br/>
<a href="https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf" target="_blank">Stanford Speech and Language Processing (Jurafsky & Martin)</a><br/>

If you're ready to give it a try: 
- <a href="https://jrfoster.shinyapps.io/NextWord/" target="_blank">ShinyApps.io</a> deployment of the application
  
If you want to see some code
- <a href="https://github.com/jrfoster/jhucapstone" target="_blank">GitHub</a> repo containing the code I used