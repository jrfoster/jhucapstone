<style>
.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  -moz-hyphens: none;
}
</style>


NextWord Suggester
========================================================
author: jrfoster
date: 20 February, 2017
autosize: true
transition: rotate

Background
========================================================
There are a number of instances of text and word suggestion in popular consumer and business technology today:
 - SMS and instant messaging on smart phones
 - Search engine query input

As part of the JHU Data Science Capstone, this application presents a word suggester for phrases based on a corpus of over 4 million documents consisting of blog and news articles and tweets read from the Twitter API.

The application is driven by a persistent n-gram model consisting of unigrams up to quintagrams and employs Stupid Backoff smoothing.

Data Processing and Model Generation
========================================================
Persistent model was created using the following general steps:
 - A random training set of 75% of news and blog articles and 25% of tweets was derived using rbinom
 - Numbers, punctuation, symbols, separators, hyphens, and URLs were removed and each document loaded into a Quanteda corpus
 - Map-reduce process used to generate n-grams using Quanteda for tokenizataion and Microsoft SQL Server for reducing
 - Relative frequencies computed and n-grams pruned based on frequency
 - SQL Server Integration Services and SQLite to create the persistent model

Prediction Algorithm Overview
========================================================
Generally speaking the algorithm used is as follows:
 - Based on the input length, candidate n-grams with relative frequencies are retrieved
 - Backoff multiplier is applied depending on recursion level
 - Highest ranked and scored words are returned as suggestions
 - If < 3 suggestions are found the top unigrams are included
 - Final Benchmark Test statistics
   - Overall top-3 score:     18.04 %
   - Overall top-1 precision: 13.56 %
   - Overall top-3 precision: 21.99 %
   - Average runtime:         30.76 msec

Application Overview
========================================================
The application has a small footprint, and a simple interface:
 - Approximately 100mb persistent n-gram store
 - 1.15mb memory used at runtime (per benchmark test)

![screenshot](./screenshot.png)

References
========================================================
Much of the work for this application is based on the following works:

<a href="http://www.aclweb.org/anthology/D07-1090.pdf" target="_blank">Large Language Models in Machine Translation (Brants, et al)</a><br/>
<a href="http://onepager.togaware.com/TextMiningO.pdf" target="_blank">Hands-On Data Science with R Text Mining (Williams)</a><br/>
<a href="https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf" target="_blank">Stanford Speech and Language Processing (Jurafsky & Martin)</a><br/>

If you're ready to give it a try: 
- <a href="https://jrfoster.shinyapps.io/NextWord/" target="_blank">ShinyApps.io</a> deployment of the application
  
If you want to see some code
- <a href="https://github.com/jrfoster/jhucapstone" target="_blank">GitHub</a> repo containing the code I used